diff --git a/src/llama.cpp b/src/llama.cpp
index 6719edb3..6d87bba0 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -21808,6 +21808,26 @@ static int32_t llama_chat_apply_template_internal(
         if (add_ass) {
             ss << "<|start_of_role|>assistant<|end_of_role|>\n";
         }
+    } else if (tmpl == "falcon" || (tmpl_contains("Falcon:") && tmpl_contains("message['content']"))) {
+        // Falcon
+        std::string system_prompt = "";
+        for (auto message : chat) {
+            std::string role(message->role);
+            if (role == "system") {
+                system_prompt = trim(message->content);
+            } else if (role == "user") {
+                if (!system_prompt.empty()) {
+                    ss << "\n" << system_prompt << "\n\n\n";
+                    system_prompt = "";
+                }
+                ss << "\nUser: " << trim(message->content) << "\n\n\n";
+            } else if (role == "assistant") {
+                ss << "\nAssistant: " << trim(message->content) << "\n\n\n";
+            }
+        }
+        if (add_ass) {
+            ss << "\nAssistant:";
+        }
     } else {
         // template not supported
         return -1;
